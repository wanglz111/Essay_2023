{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "singleCNN_Offensevalturk.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvOhQBLWPTC0",
        "outputId": "7d2bbd63-9322-4551-848b-e44ba7a6882f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "     \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "!pip install emoji\n",
        "import emoji\n",
        "!pip install openai\n",
        "import openai\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip install -q keras\n",
        "import keras\n",
        "from os import path\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print(accelerator)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import csv\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.8/dist-packages (2.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.8/dist-packages (0.25.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai) (1.21.6)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from openai) (4.1.1)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.8/dist-packages (from openai) (1.5.2.221124)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.8/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2022.6)\n",
            "Requirement already satisfied: types-pytz>=2022.1.1 in /usr/local/lib/python3.8/dist-packages (from pandas-stubs>=1.1.0.11->openai) (2022.6.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cu80\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6HarFFqRCv_"
      },
      "source": [
        "## Data Reading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CCKqQSlQ3ta",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef35788-b019-4aa2-a6ed-a424eb273a0a"
      },
      "source": [
        "# 导入训练集数据 土耳其语\n",
        "olid_training=pd.read_csv(\"/content/drive/MyDrive/TURKISH-DATA/offenseval-tr-training-v1.tsv\",sep=\"\\t\")\n",
        "X_train_FULL=olid_training[[\"id\",\"tweet\",\"subtask_a\"]] \n",
        "Y_train_FULL=olid_training[\"subtask_a\"]\n",
        "# 导入测试数据及结果\n",
        "X_test_FULL=pd.read_csv('/content/drive/MyDrive/TURKISH-DATA/offenseval-tr-testset-v1.tsv',sep='\\t',encoding='utf8',quoting=csv.QUOTE_NONE)\n",
        "Y_TEST=pd.read_csv('/content/drive/MyDrive/TURKISH-DATA/offenseval-tr-labela-v1.tsv',sep=',',encoding='utf8',quoting=csv.QUOTE_NONE,header=None)\n",
        "Y_TRAIN_ENCODED_FULL=[1 if i ==  'OFF' else 0 for i in Y_train_FULL]\n",
        "Y_TEST_ENCODED_FULL = [1 if i ==  'OFF' else 0 for i in Y_TEST[1]]\n",
        "\n",
        "print(\"---导入数据成功---\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---导入数据成功---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入训练集数据 英语\n",
        "olid_training=pd.read_csv(\"/content/drive/MyDrive/OLIDv1.0/olid-training-v1.0.tsv\",sep=\"\\t\")\n",
        "X_train_FULL=olid_training[[\"id\",\"tweet\",\"subtask_a\"]] \n",
        "Y_train_FULL=olid_training[\"subtask_a\"]\n",
        "# 导入测试数据及结果\n",
        "X_test_FULL=pd.read_csv('/content/drive/MyDrive/OLIDv1.0/testset-levela.tsv',sep='\\t',encoding='utf8',quoting=csv.QUOTE_NONE)\n",
        "Y_TEST=pd.read_csv('/content/drive/MyDrive/OLIDv1.0/labels-levela.csv',sep=',',encoding='utf8',quoting=csv.QUOTE_NONE,header=None)\n",
        "Y_TRAIN_ENCODED_FULL=[1 if i ==  'OFF' else 0 for i in Y_train_FULL]\n",
        "Y_TEST_ENCODED_FULL = [1 if i ==  'OFF' else 0 for i in Y_TEST[1]]\n",
        "\n",
        "print(\"---导入数据成功---\")\n"
      ],
      "metadata": {
        "id": "rNc7J_QoLG7l",
        "outputId": "3e30ef2c-eb0c-4c69-fa55-9a43284715ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---导入数据成功---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVUr2tSbRFTI"
      },
      "source": [
        "Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweet(tweet):\n",
        "    # Split tweet into tokens\n",
        "    tokenizer = TweetTokenizer()\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    # Remove URLs and mentions\n",
        "    tokens = [token for token in tokens if not token.startswith('http') and not token.startswith('@')]\n",
        "\n",
        "    # Decode emojis\n",
        "    tokens = [emoji.demojize(token) for token in tokens]\n",
        "\n",
        "    # Remove punctuation and non-alphanumeric characters\n",
        "    tokens = [token for token in tokens if token.isalnum()]\n",
        "\n",
        "    # Convert all tokens to lowercase\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    tokens = [token for token in tokens if ((token != \"url\"))]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Q80P961OH279"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4LmRLgfQ-9o"
      },
      "source": [
        "# 数据清洗\n",
        "\n",
        "filtered_tweets=[]\n",
        "for tweet in X_train_FULL[\"tweet\"]:\n",
        "    filtered_tweets.append(preprocess_tweet(tweet))\n",
        "X_train_FULL[\"tweet_initial\"] = filtered_tweets\n",
        "\n",
        "\n",
        "filtered_tweets=[]\n",
        "for tweet in X_test_FULL[\"tweet\"]:\n",
        "    filtered_tweets.append(preprocess_tweet(tweet))\n",
        "X_test_FULL[\"tweet_initial\"] = filtered_tweets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#FOR TEST\n",
        "\n",
        "z=[]\n",
        "for tweet in X_train_FULL[\"tweet_initial\"]:\n",
        "    d=\" \".join(tweet)\n",
        "    z.append(d)\n",
        "X_train_FULL[\"tweet_initial_nontoken\"]=z\n",
        "\n",
        "\n",
        "\n",
        "#FOR TEST\n",
        "\n",
        "z=[]\n",
        "for tweet in X_test_FULL[\"tweet_initial\"]:\n",
        "    d=\" \".join(tweet)\n",
        "    z.append(d)\n",
        "X_test_FULL[\"tweet_initial_nontoken\"]=z\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_FULL"
      ],
      "metadata": {
        "id": "X-x4msE1NcJX",
        "outputId": "e9386ebb-20bb-47b4-8204-fb153d53556c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                              tweet  \\\n",
              "0    15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   \n",
              "1    27014  #ConstitutionDay is revered by Conservatives, ...   \n",
              "2    30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...   \n",
              "3    13876  #Watching #Boomer getting the news that she is...   \n",
              "4    60133  #NoPasaran: Unity demo to oppose the far-right...   \n",
              "..     ...                                                ...   \n",
              "855  73439  #DespicableDems lie again about rifles. Dem Di...   \n",
              "856  25657  #MeetTheSpeakers 🙌 @USER will present in our e...   \n",
              "857  67018  3 people just unfollowed me for talking about ...   \n",
              "858  50665  #WednesdayWisdom Antifa calls the right fascis...   \n",
              "859  24583      #Kavanaugh typical #liberals , #Democrats URL   \n",
              "\n",
              "                                         tweet_initial  \\\n",
              "0    [democrats, support, antifa, muslim, brotherho...   \n",
              "1    [revered, conservatives, hated, progressives, ...   \n",
              "2                                 [first, reduces, ca]   \n",
              "3    [getting, news, still, parole, always, makes, ...   \n",
              "4                [unity, demo, oppose, enough, enough]   \n",
              "..                                                 ...   \n",
              "855  [lie, rifles, dem, distorted, law, push, kavan...   \n",
              "856  [present, event, oiw, 2018, finpact, global, i...   \n",
              "857  [3, people, unfollowed, talking, merlin, sorry...   \n",
              "858  [antifa, calls, right, fascist, reality, left,...   \n",
              "859                                          [typical]   \n",
              "\n",
              "                                tweet_initial_nontoken  \n",
              "0    democrats support antifa muslim brotherhood ms...  \n",
              "1    revered conservatives hated progressives socia...  \n",
              "2                                     first reduces ca  \n",
              "3    getting news still parole always makes smile f...  \n",
              "4                      unity demo oppose enough enough  \n",
              "..                                                 ...  \n",
              "855  lie rifles dem distorted law push kavanaugh co...  \n",
              "856  present event oiw 2018 finpact global impact f...  \n",
              "857  3 people unfollowed talking merlin sorry im st...  \n",
              "858  antifa calls right fascist reality left follow...  \n",
              "859                                            typical  \n",
              "\n",
              "[860 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a06632b9-eb9a-423a-b33a-06f625112369\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>tweet_initial</th>\n",
              "      <th>tweet_initial_nontoken</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15923</td>\n",
              "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
              "      <td>[democrats, support, antifa, muslim, brotherho...</td>\n",
              "      <td>democrats support antifa muslim brotherhood ms...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27014</td>\n",
              "      <td>#ConstitutionDay is revered by Conservatives, ...</td>\n",
              "      <td>[revered, conservatives, hated, progressives, ...</td>\n",
              "      <td>revered conservatives hated progressives socia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>30530</td>\n",
              "      <td>#FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...</td>\n",
              "      <td>[first, reduces, ca]</td>\n",
              "      <td>first reduces ca</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13876</td>\n",
              "      <td>#Watching #Boomer getting the news that she is...</td>\n",
              "      <td>[getting, news, still, parole, always, makes, ...</td>\n",
              "      <td>getting news still parole always makes smile f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60133</td>\n",
              "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
              "      <td>[unity, demo, oppose, enough, enough]</td>\n",
              "      <td>unity demo oppose enough enough</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855</th>\n",
              "      <td>73439</td>\n",
              "      <td>#DespicableDems lie again about rifles. Dem Di...</td>\n",
              "      <td>[lie, rifles, dem, distorted, law, push, kavan...</td>\n",
              "      <td>lie rifles dem distorted law push kavanaugh co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>856</th>\n",
              "      <td>25657</td>\n",
              "      <td>#MeetTheSpeakers 🙌 @USER will present in our e...</td>\n",
              "      <td>[present, event, oiw, 2018, finpact, global, i...</td>\n",
              "      <td>present event oiw 2018 finpact global impact f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>857</th>\n",
              "      <td>67018</td>\n",
              "      <td>3 people just unfollowed me for talking about ...</td>\n",
              "      <td>[3, people, unfollowed, talking, merlin, sorry...</td>\n",
              "      <td>3 people unfollowed talking merlin sorry im st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>858</th>\n",
              "      <td>50665</td>\n",
              "      <td>#WednesdayWisdom Antifa calls the right fascis...</td>\n",
              "      <td>[antifa, calls, right, fascist, reality, left,...</td>\n",
              "      <td>antifa calls right fascist reality left follow...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>859</th>\n",
              "      <td>24583</td>\n",
              "      <td>#Kavanaugh typical #liberals , #Democrats URL</td>\n",
              "      <td>[typical]</td>\n",
              "      <td>typical</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>860 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a06632b9-eb9a-423a-b33a-06f625112369')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a06632b9-eb9a-423a-b33a-06f625112369 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a06632b9-eb9a-423a-b33a-06f625112369');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XmFNtEjRWJr"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "def recall_m(true_Y, pred_Y):\n",
        "        TP = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        possible_pos = K.sum(K.round(K.clip(true_Y, 0, 1)))\n",
        "        rec = TP / (possible_pos + K.epsilon())\n",
        "        return rec\n",
        "\n",
        "def precision_m(true_Y, pred_Y):\n",
        "        true_positives = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(pred_Y, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def f1_m(true_Y, pred_Y):\n",
        "    pres = precision_m(true_Y, pred_Y)\n",
        "    rec = recall_m(true_Y, pred_Y)\n",
        "    return 2*((pres*rec)/(pres+rec+K.epsilon()))\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84q_PQ7zRrso"
      },
      "source": [
        "## Tweeter Word2vec / Custom Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IukK4-MQRv8m"
      },
      "source": [
        "fname= \"/content/drive/My Drive/Twitter/Word2Vec/w2v_model_word.vec\"\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(fname)  # you can load this saved keyedvectors model later\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBfCpzlOhbX8"
      },
      "source": [
        "# Tweeter FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKeL2xpXheOI"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import FastText\n",
        "\n",
        "word_vectors = gensim.models.FastText.load_fasttext_format('/content/drive/MyDrive/cc.en.300.bin',encoding='utf-8') # use that if you want to use fasttedxt \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0MPbhyWI9KO"
      },
      "source": [
        "# Public FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TP47sHiJBFl"
      },
      "source": [
        "word_vectors = gensim.models.FastText.load_fasttext_format('/content/drive/My Drive/OFFENSEVAL20-DATA/haber-P1_S0_L0.bin',encoding='utf-8') # use that if you want to use fasttedxt \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBg1YjQoys4O"
      },
      "source": [
        "# Public word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBnqjhEIyxEB"
      },
      "source": [
        "\n",
        "fname= \"/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(fname, binary = True)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4BMGRxqR6Bx"
      },
      "source": [
        "## Tokenizing / creating vocabulary and wordindex using keras functinalities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJcARFL7R_Au"
      },
      "source": [
        "\"\"\"\n",
        "We will use word indexes as look-up table during embedding layer.\n",
        "\"\"\"\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()  #the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
        "# tokenizer = Tokenizer(num_words=98790)  #the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
        "tokenizer.fit_on_texts(X_train_FULL[\"tweet_initial_nontoken\"])\n",
        "X_train_initial = tokenizer.texts_to_sequences(X_train_FULL[\"tweet_initial_nontoken\"])\n",
        "X_test_initial = tokenizer.texts_to_sequences(X_test_FULL[\"tweet_initial_nontoken\"])\n",
        "vocab_size_initial = len(tokenizer.word_index) + 1 \n",
        "wordIndex_initial=tokenizer.word_index # it is  index\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "max_len = 30\n",
        "\n",
        "\"\"\"\n",
        "Padding\n",
        "\n",
        "\"\"\"\n",
        "X_train_initial = pad_sequences(X_train_initial, padding='post', maxlen=max_len)\n",
        "X_test_initial = pad_sequences(X_test_initial, padding='post', maxlen=max_len)\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncWG_7yISOV9"
      },
      "source": [
        "def createEmbeddingLayer(wordIndex,not_static):\n",
        "  a=[]\n",
        "  embedding_dim=300\n",
        "  vocabulary_size=len(wordIndex)+1\n",
        "  embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
        "  missed=0\n",
        "  for word, i in wordIndex.items():\n",
        "    \n",
        "          \n",
        "      try:\n",
        "          embedding_vector = word_vectors[word] # or fast text\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "\n",
        "      except KeyError: # If word is not found in the word2vec vocabulary , assign random weights\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim)\n",
        "        missed+=1\n",
        "        a.append(word)\n",
        "\n",
        "  print('missed_words :' , missed)\n",
        "\n",
        "  custom_embedding_layer = Embedding(vocabulary_size,\n",
        "                                embedding_dim,\n",
        "                                weights=[embedding_matrix],\n",
        "                                trainable=not_static )# Controls the updating weights )\n",
        "  return custom_embedding_layer\n",
        "\n",
        "  "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import pickle\n",
        "\n",
        "def cnn(vocab_size,X_train,X_test,y_train,y_test,wordIndex,trainable):\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',\n",
        "                        min_delta=0,restore_best_weights=True,\n",
        "                        patience=5,\n",
        "                        verbose=1, mode='auto')]\n",
        "  model = Sequential()\n",
        "  model.add(createEmbeddingLayer(wordIndex,trainable))\n",
        "  model.add(Dropout(0.2))\n",
        "  #model.add(Conv1D(128, 1, activation='relu'))\n",
        "  model.add(Conv1D(128, 3, activation='relu'))\n",
        "  model.add(layers.GlobalMaxPooling1D())\n",
        "  \n",
        "  #model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(layers.Dense(100, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  ## Fit the model\n",
        "  model.summary()\n",
        "  model.fit(X_train, y_train, validation_split=0.1, epochs=20,callbacks=early_stopping,batch_size=32)\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "  print(\"cnn Training Loss: {:.4f}\".format(loss))\n",
        "  print(\"cnn Training Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"cnn Training f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"cnn Training Precision: {:.4f}\".format(precision))\n",
        "  print(\"cnn Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "  print(\"cnn Test Loss: {:.4f}\".format(loss))\n",
        "  print(\"cnn Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"cnn Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"cnn Test Precision: {:.4f}\".format(precision))\n",
        "  print(\"cnn Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  probs = model.predict(X_test, verbose=1)\n",
        "  predicted_classes = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "\n",
        "  #filename = 'finalized_model_lstm.sav'\n",
        "  #pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "  print(classification_report(y_test, predicted_classes ,digits=3 ))\n",
        "\n",
        "\n",
        "  print(\"cnn  ends..\")\n",
        "  return (predicted_classes,probs)"
      ],
      "metadata": {
        "id": "T6vUtWrY-O4P"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_FULL = np.array(X_train_FULL)\n",
        "X_test_FULL = np.array(X_test_FULL)\n",
        "Y_TRAIN_ENCODED_FULL = np.array(Y_TRAIN_ENCODED_FULL)\n",
        "Y_TEST_ENCODED_FULL = np.array(Y_TEST_ENCODED_FULL)"
      ],
      "metadata": {
        "id": "BfpadyzX-d0N"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_cnnlstm,probs_cnnlstm=cnn(vocab_size_initial,X_train_initial,X_test_initial,Y_TRAIN_ENCODED_FULL,Y_TEST_ENCODED_FULL,wordIndex_initial,False)"
      ],
      "metadata": {
        "id": "a9P75Uop-YD_",
        "outputId": "b300590d-f414-4105-9ce2-e3fe0139a01d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missed_words : 2607\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, None, 300)         5155800   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, None, 300)         0         \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, None, 128)         115328    \n",
            "                                                                 \n",
            " global_max_pooling1d_4 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100)               12900     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,284,129\n",
            "Trainable params: 128,329\n",
            "Non-trainable params: 5,155,800\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "373/373 [==============================] - 3s 5ms/step - loss: 0.5305 - acc: 0.7450 - f1_m: 0.4845 - precision_m: 0.6333 - recall_m: 0.4376 - val_loss: 0.4688 - val_acc: 0.7825 - val_f1_m: 0.6019 - val_precision_m: 0.7591 - val_recall_m: 0.5138\n",
            "Epoch 2/20\n",
            "373/373 [==============================] - 2s 5ms/step - loss: 0.4357 - acc: 0.8016 - f1_m: 0.6631 - precision_m: 0.7465 - recall_m: 0.6232 - val_loss: 0.4818 - val_acc: 0.7749 - val_f1_m: 0.5749 - val_precision_m: 0.7822 - val_recall_m: 0.4750\n",
            "Epoch 3/20\n",
            "373/373 [==============================] - 2s 5ms/step - loss: 0.3855 - acc: 0.8283 - f1_m: 0.7130 - precision_m: 0.7811 - recall_m: 0.6857 - val_loss: 0.4986 - val_acc: 0.7681 - val_f1_m: 0.6435 - val_precision_m: 0.6398 - val_recall_m: 0.6658\n",
            "Epoch 4/20\n",
            "373/373 [==============================] - 2s 4ms/step - loss: 0.3214 - acc: 0.8617 - f1_m: 0.7718 - precision_m: 0.8225 - recall_m: 0.7500 - val_loss: 0.5217 - val_acc: 0.7659 - val_f1_m: 0.6455 - val_precision_m: 0.6471 - val_recall_m: 0.6599\n",
            "Epoch 5/20\n",
            "373/373 [==============================] - 2s 4ms/step - loss: 0.2709 - acc: 0.8907 - f1_m: 0.8234 - precision_m: 0.8620 - recall_m: 0.8115 - val_loss: 0.5690 - val_acc: 0.7576 - val_f1_m: 0.5995 - val_precision_m: 0.6413 - val_recall_m: 0.5764\n",
            "Epoch 6/20\n",
            "366/373 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9113 - f1_m: 0.8568 - precision_m: 0.8867 - recall_m: 0.8472Restoring model weights from the end of the best epoch: 1.\n",
            "373/373 [==============================] - 2s 5ms/step - loss: 0.2191 - acc: 0.9111 - f1_m: 0.8564 - precision_m: 0.8858 - recall_m: 0.8470 - val_loss: 0.6897 - val_acc: 0.7553 - val_f1_m: 0.5493 - val_precision_m: 0.6924 - val_recall_m: 0.4713\n",
            "Epoch 6: early stopping\n",
            "414/414 [==============================] - 1s 3ms/step - loss: 0.4217 - acc: 0.8069 - f1_m: 0.6454 - precision_m: 0.8092 - recall_m: 0.5565\n",
            "cnn Training Loss: 0.4217\n",
            "cnn Training Accuracy: 0.8069\n",
            "cnn Training f1 score: 0.6454\n",
            "cnn Training Precision: 0.8092\n",
            "cnn Training Recall: 0.5565\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3927 - acc: 0.8314 - f1_m: 0.6091 - precision_m: 0.8005 - recall_m: 0.5064\n",
            "cnn Test Loss: 0.3927\n",
            "cnn Test Accuracy: 0.8314\n",
            "cnn Test f1 score: 0.6091\n",
            "cnn Test Precision: 0.8005\n",
            "cnn Test Recall: 0.5064\n",
            "27/27 [==============================] - 0s 2ms/step\n",
            "27/27 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.837     0.952     0.891       620\n",
            "           1      0.806     0.521     0.633       240\n",
            "\n",
            "    accuracy                          0.831       860\n",
            "   macro avg      0.822     0.736     0.762       860\n",
            "weighted avg      0.828     0.831     0.819       860\n",
            "\n",
            "cnn  ends..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import pickle\n",
        "from keras.layers import GRU\n",
        "\n",
        "def gru(vocab_size,X_train,X_test,y_train,y_test,wordIndex,trainable):\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',\n",
        "                        min_delta=0,restore_best_weights=True,\n",
        "                        patience=5,\n",
        "                        verbose=1, mode='auto')]\n",
        "  model = Sequential()\n",
        "  model.add(createEmbeddingLayer(wordIndex,trainable))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(GRU(32))\n",
        "  #model.add(Conv1D(128, 1, activation='relu'))\n",
        "  # model.add(Conv1D(128, 3, activation='relu'))\n",
        "  # model.add(layers.GlobalMaxPooling1D())\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  #model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "  # model.add(layers.Dense(100, activation='sigmoid'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.summary()\n",
        "  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  ## Fit the model\n",
        "  model.fit(X_train, y_train, validation_split=0.1, epochs=20,callbacks=early_stopping,batch_size=32)\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "  print(\"cnn Training Loss: {:.4f}\".format(loss))\n",
        "  print(\"cnn Training Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"cnn Training f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"cnn Training Precision: {:.4f}\".format(precision))\n",
        "  print(\"cnn Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "  print(\"cnn Test Loss: {:.4f}\".format(loss))\n",
        "  print(\"cnn Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"cnn Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"cnn Test Precision: {:.4f}\".format(precision))\n",
        "  print(\"cnn Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  probs = model.predict(X_test, verbose=1)\n",
        "  predicted_classes = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "\n",
        "  #filename = 'finalized_model_lstm.sav'\n",
        "  #pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "  print(classification_report(y_test, predicted_classes ,digits=3 ))\n",
        "\n",
        "\n",
        "  print(\"cnn  ends..\")\n",
        "  return (predicted_classes,probs)"
      ],
      "metadata": {
        "id": "-BZZ-qC8_I-f"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_cnnlstm,probs_cnnlstm=gru(vocab_size_initial,X_train_initial,X_test_initial,Y_TRAIN_ENCODED_FULL,Y_TEST_ENCODED_FULL,wordIndex_initial,False)"
      ],
      "metadata": {
        "id": "goC9X_lN_NJh",
        "outputId": "c433419c-d1c7-436f-96b0-c4d86cae5173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missed_words : 2607\n",
            "Epoch 1/20\n",
            "373/373 [==============================] - 6s 8ms/step - loss: 0.5743 - acc: 0.7148 - f1_m: 0.3393 - precision_m: 0.4035 - recall_m: 0.3288 - val_loss: 0.4731 - val_acc: 0.7847 - val_f1_m: 0.6597 - val_precision_m: 0.6770 - val_recall_m: 0.6602\n",
            "Epoch 2/20\n",
            "373/373 [==============================] - 2s 6ms/step - loss: 0.4790 - acc: 0.7769 - f1_m: 0.6236 - precision_m: 0.6970 - recall_m: 0.5951 - val_loss: 0.4530 - val_acc: 0.7983 - val_f1_m: 0.6563 - val_precision_m: 0.7452 - val_recall_m: 0.6026\n",
            "Epoch 3/20\n",
            "373/373 [==============================] - 2s 6ms/step - loss: 0.4531 - acc: 0.7882 - f1_m: 0.6379 - precision_m: 0.7253 - recall_m: 0.5974 - val_loss: 0.4773 - val_acc: 0.7560 - val_f1_m: 0.6637 - val_precision_m: 0.5991 - val_recall_m: 0.7608\n",
            "Epoch 4/20\n",
            "373/373 [==============================] - 2s 6ms/step - loss: 0.4464 - acc: 0.7950 - f1_m: 0.6517 - precision_m: 0.7324 - recall_m: 0.6123 - val_loss: 0.4886 - val_acc: 0.7477 - val_f1_m: 0.6621 - val_precision_m: 0.5825 - val_recall_m: 0.7854\n",
            "Epoch 5/20\n",
            "373/373 [==============================] - 2s 6ms/step - loss: 0.4394 - acc: 0.7957 - f1_m: 0.6542 - precision_m: 0.7311 - recall_m: 0.6189 - val_loss: 0.4409 - val_acc: 0.7931 - val_f1_m: 0.6551 - val_precision_m: 0.7216 - val_recall_m: 0.6163\n",
            "Epoch 6/20\n",
            "373/373 [==============================] - 2s 6ms/step - loss: 0.4268 - acc: 0.8038 - f1_m: 0.6696 - precision_m: 0.7432 - recall_m: 0.6391 - val_loss: 0.4455 - val_acc: 0.7923 - val_f1_m: 0.6283 - val_precision_m: 0.7672 - val_recall_m: 0.5536\n",
            "Epoch 7/20\n",
            "373/373 [==============================] - 2s 6ms/step - loss: 0.4175 - acc: 0.8109 - f1_m: 0.6806 - precision_m: 0.7538 - recall_m: 0.6470 - val_loss: 0.4590 - val_acc: 0.7915 - val_f1_m: 0.6325 - val_precision_m: 0.7675 - val_recall_m: 0.5585\n",
            "Epoch 8/20\n",
            "373/373 [==============================] - 2s 6ms/step - loss: 0.4064 - acc: 0.8173 - f1_m: 0.6975 - precision_m: 0.7603 - recall_m: 0.6693 - val_loss: 0.4468 - val_acc: 0.7885 - val_f1_m: 0.6408 - val_precision_m: 0.7371 - val_recall_m: 0.5859\n",
            "Epoch 9/20\n",
            "373/373 [==============================] - 2s 6ms/step - loss: 0.3952 - acc: 0.8248 - f1_m: 0.7100 - precision_m: 0.7676 - recall_m: 0.6837 - val_loss: 0.4519 - val_acc: 0.7878 - val_f1_m: 0.6496 - val_precision_m: 0.7119 - val_recall_m: 0.6177\n",
            "Epoch 10/20\n",
            "372/373 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.8346 - f1_m: 0.7269 - precision_m: 0.7764 - recall_m: 0.7063Restoring model weights from the end of the best epoch: 5.\n",
            "373/373 [==============================] - 2s 6ms/step - loss: 0.3816 - acc: 0.8345 - f1_m: 0.7270 - precision_m: 0.7766 - recall_m: 0.7064 - val_loss: 0.4639 - val_acc: 0.7817 - val_f1_m: 0.6575 - val_precision_m: 0.6793 - val_recall_m: 0.6558\n",
            "Epoch 10: early stopping\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, None, 300)         5155800   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, None, 300)         0         \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 32)                32064     \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 256)               8448      \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,196,569\n",
            "Trainable params: 40,769\n",
            "Non-trainable params: 5,155,800\n",
            "_________________________________________________________________\n",
            "414/414 [==============================] - 2s 4ms/step - loss: 0.4094 - acc: 0.8146 - f1_m: 0.6891 - precision_m: 0.7622 - recall_m: 0.6482\n",
            "cnn Training Loss: 0.4094\n",
            "cnn Training Accuracy: 0.8146\n",
            "cnn Training f1 score: 0.6891\n",
            "cnn Training Precision: 0.7622\n",
            "cnn Training Recall: 0.6482\n",
            "27/27 [==============================] - 0s 4ms/step - loss: 0.3947 - acc: 0.8279 - f1_m: 0.6156 - precision_m: 0.7439 - recall_m: 0.5444\n",
            "cnn Test Loss: 0.3947\n",
            "cnn Test Accuracy: 0.8279\n",
            "cnn Test f1 score: 0.6156\n",
            "cnn Test Precision: 0.7439\n",
            "cnn Test Recall: 0.5444\n",
            "27/27 [==============================] - 0s 2ms/step\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.847     0.929     0.886       620\n",
            "           1      0.756     0.567     0.648       240\n",
            "\n",
            "    accuracy                          0.828       860\n",
            "   macro avg      0.801     0.748     0.767       860\n",
            "weighted avg      0.822     0.828     0.820       860\n",
            "\n",
            "cnn  ends..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Embedding, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.layers import Bidirectional,LSTM\n",
        "\n",
        "def sigmoid(x): \n",
        "    return 1.0/(1 + np.exp(-x))\n",
        "\n",
        "\n",
        "class biLSTM(object):\n",
        "    def __init__(self, maxlen, embedding_dims,\n",
        "                 class_num=1,\n",
        "                 last_activation='sigmoid'):\n",
        "        self.maxlen = maxlen\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.class_num = class_num\n",
        "        self.last_activation = last_activation\n",
        "\n",
        "    def get_model(self):\n",
        "        input = Input((self.maxlen,))\n",
        "        embedding = createEmbeddingLayer(wordIndex_initial,False)(input) #change\n",
        "        x = Bidirectional(LSTM(128, return_sequences=False))(embedding)  # LSTM or GRU\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "\n",
        "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
        "        model = Model(inputs=input, outputs=output)\n",
        "        return model\n",
        "\n",
        "def bilstm(X_train_initial,X_test_initial,y_train,y_test) : \n",
        "  LSTM_MODEL= biLSTM(30, 100).get_model()\n",
        "\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',min_delta=0,restore_best_weights=True, patience=10,verbose=1, mode='auto')]\n",
        "  LSTM_MODEL.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  LSTM_MODEL.summary()\n",
        "\n",
        "  LSTM_MODEL.fit(X_train_initial, y_train,\n",
        "            batch_size=16,\n",
        "            epochs=30,\n",
        "            callbacks=early_stopping,\n",
        "            validation_split=0.1)\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = LSTM_MODEL.evaluate(X_train_initial, y_train, verbose=1)\n",
        "  print(\" biLSTM Training Loss: {:.4f}\".format(loss))\n",
        "  print(\" biLSTM Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"  biLSTM  f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"Attention- BiRNN Precision: {:.4f}\".format(precision))\n",
        "  print(\"Attention- BiRNN Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = LSTM_MODEL.evaluate(X_test_initial, y_test, verbose=1)\n",
        "  print(\" biLSTM Test Loss: {:.4f}\".format(loss))\n",
        "  print(\" biLSTM Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"  biLSTM Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\" biLSTM  Test Precision: {:.4f}\".format(precision))\n",
        "  print(\" biLSTM Test Recall: {:.4f}\".format(recall))\n",
        " \n",
        "  probs = LSTM_MODEL.predict(X_test_initial, verbose=1)\n",
        "  print('lenght of probs : ' ,len(probs))\n",
        "  predicted_classes=[0 if i < 0.5 else 1 for i in probs]\n",
        " \n",
        "\n",
        "  print(classification_report(y_test, predicted_classes,digits=3))\n",
        "  return (predicted_classes,probs)\n"
      ],
      "metadata": {
        "id": "lGuoWOnG_4S_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_cnnlstm,probs_cnnlstm=bilstm(X_train_initial,X_test_initial,Y_TRAIN_ENCODED_FULL,Y_TEST_ENCODED_FULL)"
      ],
      "metadata": {
        "id": "mNtX_13d_7Zh",
        "outputId": "0d22bb7c-1beb-4afe-e5be-b3a7aff16f98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missed_words : 2607\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 30)]              0         \n",
            "                                                                 \n",
            " embedding_6 (Embedding)     (None, 30, 300)           5155800   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 256)              439296    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,628,121\n",
            "Trainable params: 472,321\n",
            "Non-trainable params: 5,155,800\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "745/745 [==============================] - 10s 9ms/step - loss: 0.5068 - acc: 0.7561 - f1_m: 0.5398 - precision_m: 0.6665 - recall_m: 0.5044 - val_loss: 0.4720 - val_acc: 0.7772 - val_f1_m: 0.5977 - val_precision_m: 0.6869 - val_recall_m: 0.5618\n",
            "Epoch 2/30\n",
            "745/745 [==============================] - 6s 8ms/step - loss: 0.4511 - acc: 0.7904 - f1_m: 0.6291 - precision_m: 0.7272 - recall_m: 0.6020 - val_loss: 0.4817 - val_acc: 0.7681 - val_f1_m: 0.6504 - val_precision_m: 0.6268 - val_recall_m: 0.7241\n",
            "Epoch 3/30\n",
            "745/745 [==============================] - 6s 8ms/step - loss: 0.4173 - acc: 0.8103 - f1_m: 0.6613 - precision_m: 0.7591 - recall_m: 0.6371 - val_loss: 0.4850 - val_acc: 0.7764 - val_f1_m: 0.6111 - val_precision_m: 0.6543 - val_recall_m: 0.6095\n",
            "Epoch 4/30\n",
            "745/745 [==============================] - 6s 8ms/step - loss: 0.3754 - acc: 0.8282 - f1_m: 0.7028 - precision_m: 0.7667 - recall_m: 0.6925 - val_loss: 0.5002 - val_acc: 0.7576 - val_f1_m: 0.6189 - val_precision_m: 0.6313 - val_recall_m: 0.6579\n",
            "Epoch 5/30\n",
            "745/745 [==============================] - 6s 9ms/step - loss: 0.3258 - acc: 0.8555 - f1_m: 0.7471 - precision_m: 0.7928 - recall_m: 0.7478 - val_loss: 0.5552 - val_acc: 0.7538 - val_f1_m: 0.5856 - val_precision_m: 0.6358 - val_recall_m: 0.5881\n",
            "Epoch 6/30\n",
            "745/745 [==============================] - 6s 9ms/step - loss: 0.2551 - acc: 0.8927 - f1_m: 0.8214 - precision_m: 0.8465 - recall_m: 0.8334 - val_loss: 0.6712 - val_acc: 0.7455 - val_f1_m: 0.5592 - val_precision_m: 0.6556 - val_recall_m: 0.5352\n",
            "Epoch 7/30\n",
            "745/745 [==============================] - 6s 8ms/step - loss: 0.1937 - acc: 0.9195 - f1_m: 0.8658 - precision_m: 0.8849 - recall_m: 0.8723 - val_loss: 0.8032 - val_acc: 0.7236 - val_f1_m: 0.5929 - val_precision_m: 0.5728 - val_recall_m: 0.6681\n",
            "Epoch 8/30\n",
            "745/745 [==============================] - 7s 10ms/step - loss: 0.1337 - acc: 0.9485 - f1_m: 0.9127 - precision_m: 0.9218 - recall_m: 0.9206 - val_loss: 1.0295 - val_acc: 0.7515 - val_f1_m: 0.5929 - val_precision_m: 0.6469 - val_recall_m: 0.6038\n",
            "Epoch 9/30\n",
            "745/745 [==============================] - 6s 8ms/step - loss: 0.0955 - acc: 0.9653 - f1_m: 0.9383 - precision_m: 0.9460 - recall_m: 0.9429 - val_loss: 1.1677 - val_acc: 0.7077 - val_f1_m: 0.5643 - val_precision_m: 0.5703 - val_recall_m: 0.6226\n",
            "Epoch 10/30\n",
            "745/745 [==============================] - 6s 9ms/step - loss: 0.0744 - acc: 0.9728 - f1_m: 0.9522 - precision_m: 0.9575 - recall_m: 0.9567 - val_loss: 1.3144 - val_acc: 0.7190 - val_f1_m: 0.5911 - val_precision_m: 0.5591 - val_recall_m: 0.6809\n",
            "Epoch 11/30\n",
            "744/745 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9790 - f1_m: 0.9633 - precision_m: 0.9707 - recall_m: 0.9634Restoring model weights from the end of the best epoch: 1.\n",
            "745/745 [==============================] - 6s 9ms/step - loss: 0.0563 - acc: 0.9790 - f1_m: 0.9633 - precision_m: 0.9707 - recall_m: 0.9635 - val_loss: 1.4974 - val_acc: 0.7326 - val_f1_m: 0.5746 - val_precision_m: 0.6059 - val_recall_m: 0.5890\n",
            "Epoch 11: early stopping\n",
            "414/414 [==============================] - 2s 5ms/step - loss: 0.4458 - acc: 0.7949 - f1_m: 0.6410 - precision_m: 0.7510 - recall_m: 0.5794\n",
            " biLSTM Training Loss: 0.4458\n",
            " biLSTM Accuracy: 0.7949\n",
            "  biLSTM  f1 score: 0.6410\n",
            "Attention- BiRNN Precision: 0.7510\n",
            "Attention- BiRNN Recall: 0.5794\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.4100 - acc: 0.8256 - f1_m: 0.6042 - precision_m: 0.7628 - recall_m: 0.5192\n",
            " biLSTM Test Loss: 0.4100\n",
            " biLSTM Test Accuracy: 0.8256\n",
            "  biLSTM Test f1 score: 0.6042\n",
            " biLSTM  Test Precision: 0.7628\n",
            " biLSTM Test Recall: 0.5192\n",
            "27/27 [==============================] - 1s 4ms/step\n",
            "lenght of probs :  860\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.839     0.939     0.886       620\n",
            "           1      0.771     0.533     0.631       240\n",
            "\n",
            "    accuracy                          0.826       860\n",
            "   macro avg      0.805     0.736     0.758       860\n",
            "weighted avg      0.820     0.826     0.815       860\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import pickle\n",
        "\n",
        "def lstm(vocab_size,X_train,X_test,y_train,y_test,wordIndex,trainable):\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',\n",
        "                        min_delta=0,restore_best_weights=True,\n",
        "                        patience=5,\n",
        "                        verbose=1, mode='auto')]\n",
        "  model = Sequential()\n",
        "  model.add(createEmbeddingLayer(wordIndex,trainable))\n",
        "  model.add(Dropout(0.2))\n",
        "  # model.add(Conv1D(128, 1, activation='relu'))\n",
        "  # model.add(Conv1D(128, 3, activation='relu'))\n",
        "\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "  # model.add(layers.Dense(100, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.summary()\n",
        "  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  ## Fit the model\n",
        "  model.fit(X_train, y_train, validation_split=0.1, epochs=20,callbacks=early_stopping,batch_size=32)\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "  print(\"LSTM Training Loss: {:.4f}\".format(loss))\n",
        "  print(\"LSTM Training Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"LSTM Training f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"LSTM Training Precision: {:.4f}\".format(precision))\n",
        "  print(\"LSTM Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "  print(\"LSTM Test Loss: {:.4f}\".format(loss))\n",
        "  print(\"LSTM Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"LSTM Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"LSTM Test Precision: {:.4f}\".format(precision))\n",
        "  print(\"LSTM Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  probs = model.predict(X_test_initial, verbose=1)\n",
        "  print('lenght of probs : ' ,len(probs))\n",
        "  predicted_classes=[0 if i < 0.5 else 1 for i in probs]\n",
        "\n",
        "\n",
        "  # filename = 'finalized_model_lstm.sav'\n",
        "  # pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "  print(classification_report(y_test, predicted_classes ,digits=3 ))\n",
        "\n",
        "\n",
        "  print(\"LSTM  ends..\")\n",
        "  return (predicted_classes,probs)\n",
        "\n",
        "prediction_cnnlstm,probs_cnnlstm=lstm(vocab_size_initial,X_train_initial,X_test_initial,Y_TRAIN_ENCODED_FULL,Y_TEST_ENCODED_FULL,wordIndex_initial,True)\n",
        "\n"
      ],
      "metadata": {
        "id": "56bkK4SaFb9K",
        "outputId": "c943dc0f-61c5-430b-c562-9401a9067371",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missed_words : 2607\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, None, 300)         5155800   \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, None, 300)         0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, None, 300)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,316,301\n",
            "Trainable params: 5,316,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "373/373 [==============================] - 31s 74ms/step - loss: 0.5578 - acc: 0.7257 - f1_m: 0.4099 - precision_m: 0.5891 - recall_m: 0.3613 - val_loss: 0.4666 - val_acc: 0.7832 - val_f1_m: 0.6549 - val_precision_m: 0.6829 - val_recall_m: 0.6500\n",
            "Epoch 2/20\n",
            "373/373 [==============================] - 27s 72ms/step - loss: 0.4078 - acc: 0.8227 - f1_m: 0.7051 - precision_m: 0.7802 - recall_m: 0.6724 - val_loss: 0.5113 - val_acc: 0.7900 - val_f1_m: 0.5949 - val_precision_m: 0.8122 - val_recall_m: 0.4844\n",
            "Epoch 3/20\n",
            "373/373 [==============================] - 26s 71ms/step - loss: 0.2888 - acc: 0.8851 - f1_m: 0.8167 - precision_m: 0.8453 - recall_m: 0.8066 - val_loss: 0.6133 - val_acc: 0.7621 - val_f1_m: 0.5479 - val_precision_m: 0.7341 - val_recall_m: 0.4527\n",
            "Epoch 4/20\n",
            "373/373 [==============================] - 29s 76ms/step - loss: 0.1980 - acc: 0.9260 - f1_m: 0.8847 - precision_m: 0.8947 - recall_m: 0.8855 - val_loss: 0.6173 - val_acc: 0.7545 - val_f1_m: 0.6251 - val_precision_m: 0.6247 - val_recall_m: 0.6418\n",
            "Epoch 5/20\n",
            "373/373 [==============================] - 26s 71ms/step - loss: 0.1388 - acc: 0.9508 - f1_m: 0.9213 - precision_m: 0.9268 - recall_m: 0.9240 - val_loss: 0.6902 - val_acc: 0.7560 - val_f1_m: 0.6186 - val_precision_m: 0.6420 - val_recall_m: 0.6107\n",
            "Epoch 6/20\n",
            "373/373 [==============================] - ETA: 0s - loss: 0.0982 - acc: 0.9666 - f1_m: 0.9476 - precision_m: 0.9579 - recall_m: 0.9430Restoring model weights from the end of the best epoch: 1.\n",
            "373/373 [==============================] - 27s 71ms/step - loss: 0.0982 - acc: 0.9666 - f1_m: 0.9476 - precision_m: 0.9579 - recall_m: 0.9430 - val_loss: 0.8736 - val_acc: 0.7402 - val_f1_m: 0.6319 - val_precision_m: 0.5928 - val_recall_m: 0.6978\n",
            "Epoch 6: early stopping\n",
            "414/414 [==============================] - 3s 7ms/step - loss: 0.3892 - acc: 0.8379 - f1_m: 0.7408 - precision_m: 0.7749 - recall_m: 0.7283\n",
            "LSTM Training Loss: 0.3892\n",
            "LSTM Training Accuracy: 0.8379\n",
            "LSTM Training f1 score: 0.7408\n",
            "LSTM Training Precision: 0.7749\n",
            "LSTM Training Recall: 0.7283\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.4210 - acc: 0.8221 - f1_m: 0.6100 - precision_m: 0.7147 - recall_m: 0.5563\n",
            "LSTM Test Loss: 0.4210\n",
            "LSTM Test Accuracy: 0.8221\n",
            "LSTM Test f1 score: 0.6100\n",
            "LSTM Test Precision: 0.7147\n",
            "LSTM Test Recall: 0.5563\n",
            "27/27 [==============================] - 0s 5ms/step\n",
            "lenght of probs :  860\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.848     0.918     0.881       620\n",
            "           1      0.730     0.575     0.643       240\n",
            "\n",
            "    accuracy                          0.822       860\n",
            "   macro avg      0.789     0.746     0.762       860\n",
            "weighted avg      0.815     0.822     0.815       860\n",
            "\n",
            "LSTM  ends..\n"
          ]
        }
      ]
    }
  ]
}
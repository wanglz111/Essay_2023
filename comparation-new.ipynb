{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "singleCNN_Offensevalturk.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvOhQBLWPTC0",
        "outputId": "18a4422a-0cf9-49d6-a72f-28695856786f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "     \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!pip install -q keras\n",
        "import keras\n",
        "\n",
        "from os import path\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "print(accelerator)\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import csv\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.8/dist-packages (2.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cu80\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6HarFFqRCv_"
      },
      "source": [
        "## Data Reading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入训练集数据 英语\n",
        "olid_training=pd.read_csv(\"/content/drive/MyDrive/OLIDv1.0/olid-training-v1.0.tsv\",sep=\"\\t\")\n",
        "X_train_FULL=olid_training[[\"id\",\"tweet\",\"subtask_a\"]] \n",
        "Y_train_FULL=olid_training[\"subtask_a\"]\n",
        "# 导入测试数据及结果\n",
        "X_test_FULL=pd.read_csv('/content/drive/MyDrive/OLIDv1.0/testset-levela.tsv',sep='\\t',encoding='utf8',quoting=csv.QUOTE_NONE)\n",
        "Y_TEST=pd.read_csv('/content/drive/MyDrive/OLIDv1.0/labels-levela.csv',sep=',',encoding='utf8',quoting=csv.QUOTE_NONE,header=None)\n",
        "Y_TRAIN_ENCODED_FULL=[1 if i ==  'OFF' else 0 for i in Y_train_FULL]\n",
        "Y_TEST_ENCODED_FULL = [1 if i ==  'OFF' else 0 for i in Y_TEST[1]]\n",
        "\n",
        "print(\"---导入数据成功---\")\n"
      ],
      "metadata": {
        "id": "rNc7J_QoLG7l",
        "outputId": "2380607a-9f0e-4933-b61e-f7e5ab923cf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---导入数据成功---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVUr2tSbRFTI"
      },
      "source": [
        "Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweet(tweet):\n",
        "    # Split tweet into tokens\n",
        "    tokenizer = TweetTokenizer()\n",
        "    tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    # Remove URLs and mentions\n",
        "    tokens = [token for token in tokens if not token.startswith('http') and not token.startswith('@')]\n",
        "\n",
        "    # Decode emojis\n",
        "    tokens = [emoji.demojize(token) for token in tokens]\n",
        "\n",
        "    # Remove punctuation and non-alphanumeric characters\n",
        "    tokens = [token for token in tokens if token.isalnum()]\n",
        "\n",
        "    # Convert all tokens to lowercase\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    tokens = [token for token in tokens if ((token != \"url\"))]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Q80P961OH279"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4LmRLgfQ-9o"
      },
      "source": [
        "# 数据清洗\n",
        "\n",
        "filtered_tweets=[]\n",
        "for tweet in X_train_FULL[\"tweet\"]:\n",
        "    filtered_tweets.append(preprocess_tweet(tweet))\n",
        "X_train_FULL[\"tweet_initial\"] = filtered_tweets\n",
        "\n",
        "\n",
        "filtered_tweets=[]\n",
        "for tweet in X_test_FULL[\"tweet\"]:\n",
        "    filtered_tweets.append(preprocess_tweet(tweet))\n",
        "X_test_FULL[\"tweet_initial\"] = filtered_tweets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#FOR TEST\n",
        "\n",
        "z=[]\n",
        "for tweet in X_train_FULL[\"tweet_initial\"]:\n",
        "    d=\" \".join(tweet)\n",
        "    z.append(d)\n",
        "X_train_FULL[\"tweet_initial_nontoken\"]=z\n",
        "\n",
        "\n",
        "\n",
        "#FOR TEST\n",
        "\n",
        "z=[]\n",
        "for tweet in X_test_FULL[\"tweet_initial\"]:\n",
        "    d=\" \".join(tweet)\n",
        "    z.append(d)\n",
        "X_test_FULL[\"tweet_initial_nontoken\"]=z\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XmFNtEjRWJr"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "def recall_m(true_Y, pred_Y):\n",
        "        TP = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        possible_pos = K.sum(K.round(K.clip(true_Y, 0, 1)))\n",
        "        rec = TP / (possible_pos + K.epsilon())\n",
        "        return rec\n",
        "\n",
        "def precision_m(true_Y, pred_Y):\n",
        "        true_positives = K.sum(K.round(K.clip(true_Y * pred_Y, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(pred_Y, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def f1_m(true_Y, pred_Y):\n",
        "    pres = precision_m(true_Y, pred_Y)\n",
        "    rec = recall_m(true_Y, pred_Y)\n",
        "    return 2*((pres*rec)/(pres+rec+K.epsilon()))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WordEmbedding"
      ],
      "metadata": {
        "id": "JBq8YrRPOLvY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84q_PQ7zRrso"
      },
      "source": [
        "## Tweeter Word2vec / Custom Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IukK4-MQRv8m"
      },
      "source": [
        "fname= \"/content/drive/My Drive/Twitter/Word2Vec/w2v_model_word.vec\"\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(fname)  # you can load this saved keyedvectors model later\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBfCpzlOhbX8"
      },
      "source": [
        "## Tweeter FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKeL2xpXheOI"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import FastText\n",
        "\n",
        "word_vectors = gensim.models.FastText.load_fasttext_format('/content/drive/MyDrive/cc.en.300.bin',encoding='utf-8') # use that if you want to use fasttedxt \n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0MPbhyWI9KO"
      },
      "source": [
        "## Public FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TP47sHiJBFl"
      },
      "source": [
        "word_vectors = gensim.models.FastText.load_fasttext_format('/content/drive/My Drive/OFFENSEVAL20-DATA/haber-P1_S0_L0.bin',encoding='utf-8') # use that if you want to use fasttedxt \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBg1YjQoys4O"
      },
      "source": [
        "## Public word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBnqjhEIyxEB"
      },
      "source": [
        "\n",
        "fname= \"/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz\"\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format(fname, binary = True)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4BMGRxqR6Bx"
      },
      "source": [
        "# Tokenizing / creating vocabulary and wordindex using keras functinalities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJcARFL7R_Au"
      },
      "source": [
        "\"\"\"\n",
        "We will use word indexes as look-up table during embedding layer.\n",
        "\"\"\"\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()  #the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
        "# tokenizer = Tokenizer(num_words=98790)  #the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
        "tokenizer.fit_on_texts(X_train_FULL[\"tweet_initial_nontoken\"])\n",
        "X_train_initial = tokenizer.texts_to_sequences(X_train_FULL[\"tweet_initial_nontoken\"])\n",
        "X_test_initial = tokenizer.texts_to_sequences(X_test_FULL[\"tweet_initial_nontoken\"])\n",
        "vocab_size_initial = len(tokenizer.word_index) + 1 \n",
        "wordIndex_initial=tokenizer.word_index # it is  index\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "max_len = 30\n",
        "\n",
        "\"\"\"\n",
        "Padding\n",
        "\n",
        "\"\"\"\n",
        "X_train_initial = pad_sequences(X_train_initial, padding='post', maxlen=max_len)\n",
        "X_test_initial = pad_sequences(X_test_initial, padding='post', maxlen=max_len)\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncWG_7yISOV9"
      },
      "source": [
        "def createEmbeddingLayer(wordIndex,not_static):\n",
        "  a=[]\n",
        "  embedding_dim=300\n",
        "  vocabulary_size=len(wordIndex)+1\n",
        "  embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
        "  missed=0\n",
        "  for word, i in wordIndex.items():\n",
        "    \n",
        "          \n",
        "      try:\n",
        "          embedding_vector = word_vectors[word] # or fast text\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "\n",
        "      except KeyError: # If word is not found in the word2vec vocabulary , assign random weights\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dim)\n",
        "        missed+=1\n",
        "        a.append(word)\n",
        "\n",
        "  print('missed_words :' , missed)\n",
        "\n",
        "  custom_embedding_layer = Embedding(vocabulary_size,\n",
        "                                embedding_dim,\n",
        "                                weights=[embedding_matrix],\n",
        "                                trainable=not_static )# Controls the updating weights )\n",
        "  return custom_embedding_layer\n",
        "\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "mfNGM4X9Ob-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "BcPElFj-Mo0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_FULL = np.array(X_train_FULL)\n",
        "X_test_FULL = np.array(X_test_FULL)\n",
        "Y_TRAIN_ENCODED_FULL = np.array(Y_TRAIN_ENCODED_FULL)\n",
        "Y_TEST_ENCODED_FULL = np.array(Y_TEST_ENCODED_FULL)"
      ],
      "metadata": {
        "id": "BfpadyzX-d0N"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import pickle\n",
        "\n",
        "def cnn(vocab_size,X_train,X_test,y_train,y_test,wordIndex,trainable):\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',\n",
        "                        min_delta=0,restore_best_weights=True,\n",
        "                        patience=5,\n",
        "                        verbose=1, mode='auto')]\n",
        "  model = Sequential()\n",
        "  model.add(createEmbeddingLayer(wordIndex,trainable))\n",
        "  model.add(Dropout(0.2))\n",
        "  #model.add(Conv1D(128, 1, activation='relu'))\n",
        "  model.add(Conv1D(128, 3, activation='relu'))\n",
        "  model.add(layers.GlobalMaxPooling1D())\n",
        "  \n",
        "  #model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(layers.Dense(100, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  ## Fit the model\n",
        "  model.summary()\n",
        "  model.fit(X_train, y_train, validation_split=0.1, epochs=20,callbacks=early_stopping,batch_size=32)\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "  print(\"cnn Training Loss: {:.4f}\".format(loss))\n",
        "  print(\"cnn Training Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"cnn Training f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"cnn Training Precision: {:.4f}\".format(precision))\n",
        "  print(\"cnn Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "  print(\"cnn Test Loss: {:.4f}\".format(loss))\n",
        "  print(\"cnn Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"cnn Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"cnn Test Precision: {:.4f}\".format(precision))\n",
        "  print(\"cnn Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  probs = model.predict(X_test, verbose=1)\n",
        "  predicted_classes = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "\n",
        "  #filename = 'finalized_model_lstm.sav'\n",
        "  #pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "  print(classification_report(y_test, predicted_classes ,digits=3 ))\n",
        "\n",
        "\n",
        "  print(\"cnn  ends..\")\n",
        "  return (predicted_classes,probs)"
      ],
      "metadata": {
        "id": "T6vUtWrY-O4P"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_cnnlstm,probs_cnnlstm=cnn(vocab_size_initial,X_train_initial,X_test_initial,Y_TRAIN_ENCODED_FULL,Y_TEST_ENCODED_FULL,wordIndex_initial,True)"
      ],
      "metadata": {
        "id": "a9P75Uop-YD_",
        "outputId": "d3fec54a-3a8c-485b-b7cc-005b63f79ee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missed_words : 2607\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, None, 300)         5155800   \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, None, 300)         0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, None, 128)         115328    \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 100)               12900     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,284,129\n",
            "Trainable params: 5,284,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "373/373 [==============================] - 4s 8ms/step - loss: 0.5113 - acc: 0.7549 - f1_m: 0.5014 - precision_m: 0.6200 - recall_m: 0.4518 - val_loss: 0.4497 - val_acc: 0.7931 - val_f1_m: 0.6488 - val_precision_m: 0.7510 - val_recall_m: 0.5892\n",
            "Epoch 2/20\n",
            "373/373 [==============================] - 3s 7ms/step - loss: 0.3453 - acc: 0.8499 - f1_m: 0.7500 - precision_m: 0.8039 - recall_m: 0.7280 - val_loss: 0.5074 - val_acc: 0.7742 - val_f1_m: 0.6060 - val_precision_m: 0.7184 - val_recall_m: 0.5410\n",
            "Epoch 3/20\n",
            "373/373 [==============================] - 3s 8ms/step - loss: 0.1991 - acc: 0.9253 - f1_m: 0.8827 - precision_m: 0.8938 - recall_m: 0.8831 - val_loss: 0.6331 - val_acc: 0.7440 - val_f1_m: 0.6358 - val_precision_m: 0.5958 - val_recall_m: 0.6973\n",
            "Epoch 4/20\n",
            "373/373 [==============================] - 3s 7ms/step - loss: 0.1051 - acc: 0.9634 - f1_m: 0.9418 - precision_m: 0.9457 - recall_m: 0.9442 - val_loss: 0.7577 - val_acc: 0.7417 - val_f1_m: 0.6241 - val_precision_m: 0.6147 - val_recall_m: 0.6577\n",
            "Epoch 5/20\n",
            "373/373 [==============================] - 3s 7ms/step - loss: 0.0640 - acc: 0.9788 - f1_m: 0.9660 - precision_m: 0.9709 - recall_m: 0.9650 - val_loss: 0.8976 - val_acc: 0.7560 - val_f1_m: 0.6313 - val_precision_m: 0.6340 - val_recall_m: 0.6518\n",
            "Epoch 6/20\n",
            "371/373 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9852 - f1_m: 0.9764 - precision_m: 0.9765 - recall_m: 0.9787Restoring model weights from the end of the best epoch: 1.\n",
            "373/373 [==============================] - 3s 7ms/step - loss: 0.0459 - acc: 0.9851 - f1_m: 0.9763 - precision_m: 0.9762 - recall_m: 0.9788 - val_loss: 0.9586 - val_acc: 0.7447 - val_f1_m: 0.6210 - val_precision_m: 0.6052 - val_recall_m: 0.6596\n",
            "Epoch 6: early stopping\n",
            "414/414 [==============================] - 1s 3ms/step - loss: 0.3398 - acc: 0.8527 - f1_m: 0.7469 - precision_m: 0.8432 - recall_m: 0.6889\n",
            "cnn Training Loss: 0.3398\n",
            "cnn Training Accuracy: 0.8527\n",
            "cnn Training f1 score: 0.7469\n",
            "cnn Training Precision: 0.8432\n",
            "cnn Training Recall: 0.6889\n",
            "27/27 [==============================] - 0s 3ms/step - loss: 0.3892 - acc: 0.8349 - f1_m: 0.6292 - precision_m: 0.7703 - recall_m: 0.5563\n",
            "cnn Test Loss: 0.3892\n",
            "cnn Test Accuracy: 0.8349\n",
            "cnn Test f1 score: 0.6292\n",
            "cnn Test Precision: 0.7703\n",
            "cnn Test Recall: 0.5563\n",
            "27/27 [==============================] - 0s 2ms/step\n",
            "27/27 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.850     0.935     0.891       620\n",
            "           1      0.775     0.575     0.660       240\n",
            "\n",
            "    accuracy                          0.835       860\n",
            "   macro avg      0.813     0.755     0.776       860\n",
            "weighted avg      0.829     0.835     0.827       860\n",
            "\n",
            "cnn  ends..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRU"
      ],
      "metadata": {
        "id": "qINLmVZrMvtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import pickle\n",
        "from keras.layers import GRU\n",
        "\n",
        "def gru(vocab_size,X_train,X_test,y_train,y_test,wordIndex,trainable):\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',\n",
        "                        min_delta=0,restore_best_weights=True,\n",
        "                        patience=5,\n",
        "                        verbose=1, mode='auto')]\n",
        "  model = Sequential()\n",
        "  model.add(createEmbeddingLayer(wordIndex,trainable))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(GRU(32))\n",
        "  #model.add(Conv1D(128, 1, activation='relu'))\n",
        "  # model.add(Conv1D(128, 3, activation='relu'))\n",
        "  # model.add(layers.GlobalMaxPooling1D())\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  #model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "  # model.add(layers.Dense(100, activation='sigmoid'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.summary()\n",
        "  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  ## Fit the model\n",
        "  model.fit(X_train, y_train, validation_split=0.1, epochs=20,callbacks=early_stopping,batch_size=32)\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "  print(\"gru Training Loss: {:.4f}\".format(loss))\n",
        "  print(\"gru Training Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"gru Training f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"gru Training Precision: {:.4f}\".format(precision))\n",
        "  print(\"gru Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "  print(\"gru Test Loss: {:.4f}\".format(loss))\n",
        "  print(\"gru Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"gru Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"gru Test Precision: {:.4f}\".format(precision))\n",
        "  print(\"gru Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  probs = model.predict(X_test, verbose=1)\n",
        "  predicted_classes = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "\n",
        "  #filename = 'finalized_model_lstm.sav'\n",
        "  #pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "  print(classification_report(y_test, predicted_classes ,digits=3 ))\n",
        "\n",
        "\n",
        "  print(\"gru  ends..\")\n",
        "  return (predicted_classes,probs)"
      ],
      "metadata": {
        "id": "-BZZ-qC8_I-f"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_cnnlstm,probs_cnnlstm=gru(vocab_size_initial,X_train_initial,X_test_initial,Y_TRAIN_ENCODED_FULL,Y_TEST_ENCODED_FULL,wordIndex_initial,True)"
      ],
      "metadata": {
        "id": "goC9X_lN_NJh",
        "outputId": "4762ca18-240f-482c-a1f3-f9277130d4ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missed_words : 2607\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, None, 300)         5155800   \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, None, 300)         0         \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (None, 32)                32064     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 256)               8448      \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,196,569\n",
            "Trainable params: 5,196,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "373/373 [==============================] - 6s 11ms/step - loss: 0.6366 - acc: 0.6680 - f1_m: 0.0220 - precision_m: 0.1048 - recall_m: 0.0143 - val_loss: 0.6286 - val_acc: 0.6699 - val_f1_m: 0.0829 - val_precision_m: 0.4048 - val_recall_m: 0.0472\n",
            "Epoch 2/20\n",
            "373/373 [==============================] - 3s 9ms/step - loss: 0.4873 - acc: 0.7783 - f1_m: 0.6099 - precision_m: 0.7139 - recall_m: 0.5679 - val_loss: 0.4678 - val_acc: 0.7915 - val_f1_m: 0.5817 - val_precision_m: 0.8637 - val_recall_m: 0.4514\n",
            "Epoch 3/20\n",
            "373/373 [==============================] - 3s 8ms/step - loss: 0.3414 - acc: 0.8601 - f1_m: 0.7730 - precision_m: 0.8080 - recall_m: 0.7599 - val_loss: 0.5004 - val_acc: 0.7772 - val_f1_m: 0.6456 - val_precision_m: 0.6687 - val_recall_m: 0.6421\n",
            "Epoch 4/20\n",
            "373/373 [==============================] - 3s 9ms/step - loss: 0.2280 - acc: 0.9138 - f1_m: 0.8596 - precision_m: 0.8822 - recall_m: 0.8527 - val_loss: 0.6314 - val_acc: 0.7787 - val_f1_m: 0.6041 - val_precision_m: 0.7249 - val_recall_m: 0.5341\n",
            "Epoch 5/20\n",
            "373/373 [==============================] - 3s 9ms/step - loss: 0.1389 - acc: 0.9460 - f1_m: 0.9136 - precision_m: 0.9298 - recall_m: 0.9062 - val_loss: 0.7313 - val_acc: 0.7379 - val_f1_m: 0.6027 - val_precision_m: 0.5925 - val_recall_m: 0.6288\n",
            "Epoch 6/20\n",
            "373/373 [==============================] - 3s 9ms/step - loss: 0.0927 - acc: 0.9640 - f1_m: 0.9418 - precision_m: 0.9531 - recall_m: 0.9361 - val_loss: 1.0191 - val_acc: 0.7243 - val_f1_m: 0.6104 - val_precision_m: 0.5637 - val_recall_m: 0.6853\n",
            "Epoch 7/20\n",
            "371/373 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9739 - f1_m: 0.9580 - precision_m: 0.9680 - recall_m: 0.9528Restoring model weights from the end of the best epoch: 2.\n",
            "373/373 [==============================] - 4s 10ms/step - loss: 0.0667 - acc: 0.9740 - f1_m: 0.9582 - precision_m: 0.9682 - recall_m: 0.9531 - val_loss: 1.0413 - val_acc: 0.7341 - val_f1_m: 0.6034 - val_precision_m: 0.5897 - val_recall_m: 0.6382\n",
            "Epoch 7: early stopping\n",
            "414/414 [==============================] - 2s 5ms/step - loss: 0.3811 - acc: 0.8223 - f1_m: 0.6504 - precision_m: 0.8884 - recall_m: 0.5323\n",
            "gru Training Loss: 0.3811\n",
            "gru Training Accuracy: 0.8223\n",
            "gru Training f1 score: 0.6504\n",
            "gru Training Precision: 0.8884\n",
            "gru Training Recall: 0.5323\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.4158 - acc: 0.8302 - f1_m: 0.5771 - precision_m: 0.8786 - recall_m: 0.4529\n",
            "gru Test Loss: 0.4158\n",
            "gru Test Accuracy: 0.8302\n",
            "gru Test f1 score: 0.5771\n",
            "gru Test Precision: 0.8786\n",
            "gru Test Recall: 0.4529\n",
            "27/27 [==============================] - 1s 3ms/step\n",
            "27/27 [==============================] - 0s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.823     0.974     0.892       620\n",
            "           1      0.873     0.458     0.601       240\n",
            "\n",
            "    accuracy                          0.830       860\n",
            "   macro avg      0.848     0.716     0.747       860\n",
            "weighted avg      0.837     0.830     0.811       860\n",
            "\n",
            "gru  ends..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "NeP7eULUM88i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import pickle\n",
        "\n",
        "def lstm(vocab_size,X_train,X_test,y_train,y_test,wordIndex,trainable):\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',\n",
        "                        min_delta=0,restore_best_weights=True,\n",
        "                        patience=5,\n",
        "                        verbose=1, mode='auto')]\n",
        "  model = Sequential()\n",
        "  model.add(createEmbeddingLayer(wordIndex,trainable))\n",
        "  model.add(Dropout(0.2))\n",
        "  # model.add(Conv1D(128, 1, activation='relu'))\n",
        "  # model.add(Conv1D(128, 3, activation='relu'))\n",
        "\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "  # model.add(layers.Dense(100, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.summary()\n",
        "  model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), metrics=['acc',f1_m,precision_m, recall_m])\n",
        "  ## Fit the model\n",
        "  model.fit(X_train, y_train, validation_split=0.1, epochs=20,callbacks=early_stopping,batch_size=32)\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_train, y_train, verbose=1)\n",
        "  print(\"LSTM Training Loss: {:.4f}\".format(loss))\n",
        "  print(\"LSTM Training Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"LSTM Training f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"LSTM Training Precision: {:.4f}\".format(precision))\n",
        "  print(\"LSTM Training Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=1)\n",
        "  print(\"LSTM Test Loss: {:.4f}\".format(loss))\n",
        "  print(\"LSTM Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"LSTM Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"LSTM Test Precision: {:.4f}\".format(precision))\n",
        "  print(\"LSTM Test Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  probs = model.predict(X_test_initial, verbose=1)\n",
        "  print('lenght of probs : ' ,len(probs))\n",
        "  predicted_classes=[0 if i < 0.5 else 1 for i in probs]\n",
        "\n",
        "\n",
        "  # filename = 'finalized_model_lstm.sav'\n",
        "  # pickle.dump(model, open(filename, 'wb'))\n",
        "\n",
        "  print(classification_report(y_test, predicted_classes ,digits=3 ))\n",
        "\n",
        "\n",
        "  print(\"LSTM  ends..\")\n",
        "  return (predicted_classes,probs)"
      ],
      "metadata": {
        "id": "56bkK4SaFb9K"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_cnnlstm,probs_cnnlstm=lstm(vocab_size_initial,X_train_initial,X_test_initial,Y_TRAIN_ENCODED_FULL,Y_TEST_ENCODED_FULL,wordIndex_initial,True)"
      ],
      "metadata": {
        "id": "u_PcxkbaH2V2",
        "outputId": "ff956519-bdc7-4227-d768-9da69d4f9cc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missed_words : 2607\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, None, 300)         5155800   \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, None, 300)         0         \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, None, 300)        0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,316,301\n",
            "Trainable params: 5,316,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "373/373 [==============================] - 32s 74ms/step - loss: 0.5484 - acc: 0.7281 - f1_m: 0.4349 - precision_m: 0.5878 - recall_m: 0.3918 - val_loss: 0.4768 - val_acc: 0.7855 - val_f1_m: 0.6679 - val_precision_m: 0.6800 - val_recall_m: 0.6745\n",
            "Epoch 2/20\n",
            "373/373 [==============================] - 27s 74ms/step - loss: 0.4014 - acc: 0.8251 - f1_m: 0.7103 - precision_m: 0.7690 - recall_m: 0.6858 - val_loss: 0.4700 - val_acc: 0.7976 - val_f1_m: 0.6348 - val_precision_m: 0.7720 - val_recall_m: 0.5540\n",
            "Epoch 3/20\n",
            "373/373 [==============================] - 28s 74ms/step - loss: 0.2860 - acc: 0.8891 - f1_m: 0.8232 - precision_m: 0.8476 - recall_m: 0.8209 - val_loss: 0.5529 - val_acc: 0.7674 - val_f1_m: 0.6232 - val_precision_m: 0.6639 - val_recall_m: 0.6108\n",
            "Epoch 4/20\n",
            "373/373 [==============================] - 27s 73ms/step - loss: 0.1918 - acc: 0.9283 - f1_m: 0.8858 - precision_m: 0.8937 - recall_m: 0.8912 - val_loss: 0.6620 - val_acc: 0.7560 - val_f1_m: 0.6221 - val_precision_m: 0.6242 - val_recall_m: 0.6407\n",
            "Epoch 5/20\n",
            "373/373 [==============================] - 27s 72ms/step - loss: 0.1312 - acc: 0.9522 - f1_m: 0.9250 - precision_m: 0.9282 - recall_m: 0.9292 - val_loss: 0.8049 - val_acc: 0.7508 - val_f1_m: 0.6050 - val_precision_m: 0.6321 - val_recall_m: 0.6021\n",
            "Epoch 6/20\n",
            "373/373 [==============================] - 29s 79ms/step - loss: 0.1003 - acc: 0.9674 - f1_m: 0.9485 - precision_m: 0.9501 - recall_m: 0.9524 - val_loss: 0.8928 - val_acc: 0.7266 - val_f1_m: 0.6050 - val_precision_m: 0.5792 - val_recall_m: 0.6589\n",
            "Epoch 7/20\n",
            "373/373 [==============================] - ETA: 0s - loss: 0.0798 - acc: 0.9718 - f1_m: 0.9554 - precision_m: 0.9589 - recall_m: 0.9567Restoring model weights from the end of the best epoch: 2.\n",
            "373/373 [==============================] - 28s 76ms/step - loss: 0.0798 - acc: 0.9718 - f1_m: 0.9554 - precision_m: 0.9589 - recall_m: 0.9567 - val_loss: 0.9426 - val_acc: 0.7372 - val_f1_m: 0.6195 - val_precision_m: 0.5913 - val_recall_m: 0.6713\n",
            "Epoch 7: early stopping\n",
            "414/414 [==============================] - 3s 7ms/step - loss: 0.2976 - acc: 0.8850 - f1_m: 0.7988 - precision_m: 0.9076 - recall_m: 0.7270\n",
            "LSTM Training Loss: 0.2976\n",
            "LSTM Training Accuracy: 0.8850\n",
            "LSTM Training f1 score: 0.7988\n",
            "LSTM Training Precision: 0.9076\n",
            "LSTM Training Recall: 0.7270\n",
            "27/27 [==============================] - 0s 7ms/step - loss: 0.4176 - acc: 0.8302 - f1_m: 0.5976 - precision_m: 0.8216 - recall_m: 0.4883\n",
            "LSTM Test Loss: 0.4176\n",
            "LSTM Test Accuracy: 0.8302\n",
            "LSTM Test f1 score: 0.5976\n",
            "LSTM Test Precision: 0.8216\n",
            "LSTM Test Recall: 0.4883\n",
            "27/27 [==============================] - 0s 6ms/step\n",
            "lenght of probs :  860\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.832     0.958     0.891       620\n",
            "           1      0.822     0.500     0.622       240\n",
            "\n",
            "    accuracy                          0.830       860\n",
            "   macro avg      0.827     0.729     0.756       860\n",
            "weighted avg      0.829     0.830     0.816       860\n",
            "\n",
            "LSTM  ends..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bi-LSTM"
      ],
      "metadata": {
        "id": "2S8iME-ONBGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Bidirectional,LSTM\n",
        "\n",
        "def bilstm(vocab_size,X_train_initial,X_test_initial,y_train,y_test,wordIndex,trainable) : \n",
        "  model = Sequential()\n",
        "  model.add(createEmbeddingLayer(wordIndex,trainable))\n",
        "  model.add(Bidirectional(LSTM(128, return_sequences=False)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.summary()\n",
        "\n",
        "  early_stopping = [EarlyStopping(monitor='val_loss',min_delta=0,restore_best_weights=True, patience=10,verbose=1, mode='auto')]\n",
        "  model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "  model.fit(X_train_initial, y_train,\n",
        "            batch_size=16,\n",
        "            epochs=30,\n",
        "            callbacks=early_stopping,\n",
        "            validation_split=0.1)\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_train_initial, y_train, verbose=1)\n",
        "  print(\" biLSTM Training Loss: {:.4f}\".format(loss))\n",
        "  print(\" biLSTM Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"  biLSTM  f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\"biLSTM  Precision: {:.4f}\".format(precision))\n",
        "  print(\"biLSTM  Recall: {:.4f}\".format(recall))\n",
        "\n",
        "  loss, accuracy, f1_score, precision, recall = model.evaluate(X_test_initial, y_test, verbose=1)\n",
        "  print(\" biLSTM Test Loss: {:.4f}\".format(loss))\n",
        "  print(\" biLSTM Test Accuracy: {:.4f}\".format(accuracy))\n",
        "  print(\"  biLSTM Test f1 score: {:.4f}\".format(f1_score))\n",
        "  print(\" biLSTM  Test Precision: {:.4f}\".format(precision))\n",
        "  print(\" biLSTM Test Recall: {:.4f}\".format(recall))\n",
        " \n",
        "  probs = model.predict(X_test_initial, verbose=1)\n",
        "  print('lenght of probs : ' ,len(probs))\n",
        "  predicted_classes=[0 if i < 0.5 else 1 for i in probs]\n",
        " \n",
        "\n",
        "  print(classification_report(y_test, predicted_classes,digits=3))\n",
        "  return (predicted_classes,probs)\n"
      ],
      "metadata": {
        "id": "y8iMxmMMIDrB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_cnnlstm,probs_cnnlstm=bilstm(vocab_size_initial,X_train_initial,X_test_initial,Y_TRAIN_ENCODED_FULL,Y_TEST_ENCODED_FULL,wordIndex_initial,True)"
      ],
      "metadata": {
        "id": "mNtX_13d_7Zh",
        "outputId": "d62f28c8-bdc8-4683-f890-b3409f6fbe3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "missed_words : 2607\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, None, 300)         5155800   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 256)              439296    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,628,121\n",
            "Trainable params: 5,628,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "745/745 [==============================] - 12s 12ms/step - loss: 0.5023 - acc: 0.7590 - f1_m: 0.5344 - precision_m: 0.6475 - recall_m: 0.5035 - val_loss: 0.4662 - val_acc: 0.7795 - val_f1_m: 0.5780 - val_precision_m: 0.7040 - val_recall_m: 0.5253\n",
            "Epoch 2/30\n",
            "745/745 [==============================] - 8s 11ms/step - loss: 0.3416 - acc: 0.8490 - f1_m: 0.7430 - precision_m: 0.7869 - recall_m: 0.7452 - val_loss: 0.5056 - val_acc: 0.7757 - val_f1_m: 0.6208 - val_precision_m: 0.6678 - val_recall_m: 0.6150\n",
            "Epoch 3/30\n",
            "745/745 [==============================] - 8s 11ms/step - loss: 0.1945 - acc: 0.9204 - f1_m: 0.8653 - precision_m: 0.8846 - recall_m: 0.8727 - val_loss: 0.6938 - val_acc: 0.7432 - val_f1_m: 0.5931 - val_precision_m: 0.6281 - val_recall_m: 0.6006\n",
            "Epoch 4/30\n",
            "745/745 [==============================] - 8s 11ms/step - loss: 0.1056 - acc: 0.9598 - f1_m: 0.9312 - precision_m: 0.9356 - recall_m: 0.9401 - val_loss: 1.1118 - val_acc: 0.7273 - val_f1_m: 0.6021 - val_precision_m: 0.5813 - val_recall_m: 0.6771\n",
            "Epoch 5/30\n",
            "745/745 [==============================] - 8s 11ms/step - loss: 0.0568 - acc: 0.9781 - f1_m: 0.9609 - precision_m: 0.9673 - recall_m: 0.9630 - val_loss: 1.3288 - val_acc: 0.7243 - val_f1_m: 0.5591 - val_precision_m: 0.5893 - val_recall_m: 0.5854\n",
            "Epoch 6/30\n",
            "745/745 [==============================] - 9s 12ms/step - loss: 0.0384 - acc: 0.9858 - f1_m: 0.9762 - precision_m: 0.9790 - recall_m: 0.9780 - val_loss: 1.3747 - val_acc: 0.7394 - val_f1_m: 0.5694 - val_precision_m: 0.6146 - val_recall_m: 0.5740\n",
            "Epoch 7/30\n",
            "745/745 [==============================] - 8s 11ms/step - loss: 0.0358 - acc: 0.9862 - f1_m: 0.9788 - precision_m: 0.9845 - recall_m: 0.9773 - val_loss: 1.5928 - val_acc: 0.7281 - val_f1_m: 0.5686 - val_precision_m: 0.5950 - val_recall_m: 0.5962\n",
            "Epoch 8/30\n",
            "745/745 [==============================] - 8s 11ms/step - loss: 0.0259 - acc: 0.9906 - f1_m: 0.9835 - precision_m: 0.9871 - recall_m: 0.9829 - val_loss: 1.6269 - val_acc: 0.7137 - val_f1_m: 0.5681 - val_precision_m: 0.5555 - val_recall_m: 0.6313\n",
            "Epoch 9/30\n",
            "745/745 [==============================] - 9s 12ms/step - loss: 0.0192 - acc: 0.9920 - f1_m: 0.9845 - precision_m: 0.9873 - recall_m: 0.9842 - val_loss: 2.0073 - val_acc: 0.7085 - val_f1_m: 0.5691 - val_precision_m: 0.5519 - val_recall_m: 0.6422\n",
            "Epoch 10/30\n",
            "745/745 [==============================] - 8s 11ms/step - loss: 0.0148 - acc: 0.9938 - f1_m: 0.9881 - precision_m: 0.9932 - recall_m: 0.9855 - val_loss: 2.4117 - val_acc: 0.7032 - val_f1_m: 0.5849 - val_precision_m: 0.5482 - val_recall_m: 0.6790\n",
            "Epoch 11/30\n",
            "741/745 [============================>.] - ETA: 0s - loss: 0.0208 - acc: 0.9923 - f1_m: 0.9872 - precision_m: 0.9905 - recall_m: 0.9866Restoring model weights from the end of the best epoch: 1.\n",
            "745/745 [==============================] - 8s 11ms/step - loss: 0.0207 - acc: 0.9924 - f1_m: 0.9873 - precision_m: 0.9905 - recall_m: 0.9867 - val_loss: 1.8773 - val_acc: 0.7289 - val_f1_m: 0.5645 - val_precision_m: 0.5895 - val_recall_m: 0.5972\n",
            "Epoch 11: early stopping\n",
            "414/414 [==============================] - 2s 5ms/step - loss: 0.3864 - acc: 0.8466 - f1_m: 0.7277 - precision_m: 0.8578 - recall_m: 0.6507\n",
            " biLSTM Training Loss: 0.3864\n",
            " biLSTM Accuracy: 0.8466\n",
            "  biLSTM  f1 score: 0.7277\n",
            "biLSTM  Precision: 0.8578\n",
            "biLSTM  Recall: 0.6507\n",
            "27/27 [==============================] - 0s 5ms/step - loss: 0.4116 - acc: 0.8442 - f1_m: 0.6315 - precision_m: 0.8231 - recall_m: 0.5221\n",
            " biLSTM Test Loss: 0.4116\n",
            " biLSTM Test Accuracy: 0.8442\n",
            "  biLSTM Test f1 score: 0.6315\n",
            " biLSTM  Test Precision: 0.8231\n",
            " biLSTM Test Recall: 0.5221\n",
            "27/27 [==============================] - 1s 4ms/step\n",
            "lenght of probs :  860\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.843     0.963     0.899       620\n",
            "           1      0.849     0.537     0.658       240\n",
            "\n",
            "    accuracy                          0.844       860\n",
            "   macro avg      0.846     0.750     0.779       860\n",
            "weighted avg      0.845     0.844     0.832       860\n",
            "\n"
          ]
        }
      ]
    }
  ]
}